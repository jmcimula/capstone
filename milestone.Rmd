---
title: "Efficient Estimation of Word represention in a Corpus"
author: "Jean Marie Cimula"
date: "August 31, 2016"
output: html_document
---

```{r results='hide', message=FALSE, warning=FALSE}
#Loading libraries
library(stringi)
library(tm)
library(knitr)
library(RWeka)
library(SnowballC)

rm(list=ls())
```

#1. Data Load

```{r}
setwd("~/R/milestone/") 

TW <- file("data/en_US.twitter.txt")
BL <- file("data/en_US.blogs.txt",open = "r")
NW <- file("data/en_US.news.txt",open = "r")

#Reading the files
twitter <- readLines(TW,warn = F)
blog    <- readLines(BL,warn = F)
news    <- readLines(NW,warn = F)

#SUMMARY OF THREE FILES : Word counts, line counts and basic data tables

#Creating a vector with the length of each dataset
dtlength <- c(length(twitter), length(blog), length(news))

#Creating a vector with the size of each dataset
dataSize   <- c(    
                    file.size("data/en_US.twitter.txt") / 1024 ^ 2, 
                    file.size("data/en_US.blogs.txt")  / 1024 ^ 2, 
                    file.size("data/en_US.news.txt")  / 1024 ^ 2 
                )
#Creating a vector with the count of words of each dataset
CountWord <- c(
                   sum(stri_count_words(twitter)), 
                   sum(stri_count_words(blog)), 
                   sum(stri_count_words(news))
               )
#Close the files
close(TW)
close(NW)
close(BL)

dtcol <- c("Filename","CountWord","CountLines","FileSizeMb")
cat <- c("Twitter","Blog","News")

gpdata <- cbind(cat,CountWord,dtlength,round(dataSize,2)) #Preparing the dataframe
gpdata <- as.data.frame(gpdata) #Defining the dataframe as dataframe
colnames(gpdata) <- dtcol #Naming the column

#Printing basic summaries of the three files : Word counts, line counts, file size
kable(gpdata)
```

#2. Exploratory analysis

```{r}
#2.1. Data Cleaning

#Using the file of news
#Creating a corpus based on News source
dm <- VCorpus(DirSource(directory="data/news", encoding="UTF-8"),readerControl=list(language="en"))

#Creating n-grams from RWeka
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))

#Cleaning the corpus
dm <- tm_map(dm,content_transformer(removePunctuation))
dm <- tm_map(dm,content_transformer(stripWhitespace))
dm <- tm_map(dm,content_transformer(removeNumbers))
dm <- tm_map(dm,PlainTextDocument)
dm <- tm_map(dm,content_transformer(tolower))

# create a DocumentTermMatrix  
options(mc.cores=1)
tunigram <- DocumentTermMatrix(dm, control=list(tokenizer=unigram))
tbigram <- DocumentTermMatrix(dm, control=list(tokenizer=bigram))

#Document Term Matrix Unigram
findFreqTerms(tunigram, 100)

#Document Term Matrix Bigram 
findFreqTerms(tbigram, 50)

#2.2. Histograms to illustrate features of the data

#2.2.1. Unigrams
ugm <- inspect(tunigram)
tugm <- table(ugm)
hist (log(tugm), main="Histogram of Unigrams", breaks=50)

#2.2.2. Bigrams
bgm <- inspect(tbigram)
tbgm <- table(bgm)
hist (log(tbgm), main="Histogram of Unigrams", breaks=50)

```
